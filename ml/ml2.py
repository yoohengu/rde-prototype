"""
ê¸°ì—… ë¶€ë„ í™•ë¥  ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- í•™ìŠµ: ë¶ˆê· í˜• ë°ì´í„° + class_weight='balanced'
- ì˜ˆì¸¡: ìƒˆë¡œìš´ ê¸°ì—… ë°ì´í„° ì…ë ¥ â†’ ë¶€ë„ í™•ë¥  ì¶œë ¥
- Feature Importance ë¶„ì„ ì¶”ê°€
"""
from dotenv import load_dotenv
import os
import pandas as pd
import numpy as np
import joblib
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, accuracy_score, precision_score, recall_score, f1_score
)
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ê¸°ì—… ë¶€ë„ í™•ë¥  ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
print("=" * 80)

# ============================================================================
# PART 1: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
# ============================================================================

def train_and_save_model():
    """ëª¨ë¸ í•™ìŠµ ë° ì €ì¥"""
    
    print("\n[PART 1] ëª¨ë¸ í•™ìŠµ ë° ì €ì¥")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1] ë°ì´í„° ë¡œë“œ ì¤‘...")
    load_dotenv()
    df_path = os.getenv("DATA_ML")
    df = pd.read_csv(df_path)
    print(f"âœ“ ë°ì´í„° shape: {df.shape}")
    
    # 2. íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    target_col = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'
    print(f"\n[2] íƒ€ê²Ÿ ë³€ìˆ˜: {target_col}")
    print(f"ë¶€ë„ìœ¨: {df[target_col].mean():.2%}")
    print(f"ë¶€ë„ ê¸°ì—…: {df[target_col].sum()}ê°œ / ì „ì²´: {len(df)}ê°œ")
    
    # 3. íŠ¹ì„± ì„ íƒ
    print("\n[3] íŠ¹ì„± ì„ íƒ ë° ì „ì²˜ë¦¬...")
    
    exclude_cols = [
        'ê¸°ì¤€ë…„ì›”', 'ì—…ì¢…(ì¤‘ë¶„ë¥˜)', 'ì„¤ë¦½ì¼ì', 'ì£¼ì†Œì§€ì‹œêµ°êµ¬',
        target_col, 'ê¸°ì—…ì‹ ìš©í‰ê°€ë“±ê¸‰(êµ¬ê°„í™”)'
    ]
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    X = df[feature_cols].copy()
    y = df[target_col].copy()
    
    print(f"ì´ˆê¸° íŠ¹ì„± ìˆ˜: {len(feature_cols)}")
    
    # 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    missing_ratio = (X.isnull().sum() / len(X) * 100)
    high_missing_cols = missing_ratio[missing_ratio > 50].index.tolist()
    X = X.drop(columns=high_missing_cols)
    print(f"ê²°ì¸¡ì¹˜ 50% ì´ìƒ ì»¬ëŸ¼ {len(high_missing_cols)}ê°œ ì œê±°")
    
    X = X.fillna(X.median())
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    print(f"ìµœì¢… íŠ¹ì„± ìˆ˜: {X.shape[1]}")
    
    # ìµœì¢… feature_cols ì €ì¥ (ì¤‘ìš”!)
    final_feature_cols = X.columns.tolist()
    
    # 5. ë°ì´í„° ë¶„í• 
    print("\n[4] ë°ì´í„° ë¶„í• ...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    
    # 6. ìŠ¤ì¼€ì¼ë§
    print("\n[5] íŠ¹ì„± ìŠ¤ì¼€ì¼ë§...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 7. ëª¨ë¸ í•™ìŠµ (Logistic Regression - ìµœì )
    print("\n[6] Logistic Regression í•™ìŠµ (class_weight='balanced')...")
    
    model = LogisticRegression(
        max_iter=1000,
        random_state=42,
        class_weight='balanced',  # í•µì‹¬!
        solver='lbfgs'
    )
    
    model.fit(X_train_scaled, y_train)
    print("âœ“ í•™ìŠµ ì™„ë£Œ")
    
    # 8. ëª¨ë¸ í‰ê°€
    print("\n[7] ëª¨ë¸ í‰ê°€...")
    
    # ê¸°ë³¸ ì„ê³„ê°’ (0.5)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = (y_pred_proba >= 0.5).astype(int)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    print(f"\nì„ê³„ê°’ 0.5 (ê¸°ë³¸):")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1 Score:  {f1:.4f}")
    print(f"  ROC AUC:   {roc_auc:.4f}")
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (Recall ìš°ì„ )
    print("\n[8] ìµœì  ì„ê³„ê°’ íƒìƒ‰...")
    best_threshold = 0.5
    best_f1 = f1
    
    for threshold in np.arange(0.1, 0.6, 0.05):
        y_pred_temp = (y_pred_proba >= threshold).astype(int)
        recall_temp = recall_score(y_test, y_pred_temp, zero_division=0)
        precision_temp = precision_score(y_test, y_pred_temp, zero_division=0)
        f1_temp = f1_score(y_test, y_pred_temp, zero_division=0)
        
        # Recallì´ 0.6 ì´ìƒì´ë©´ì„œ F1ì´ ê°€ì¥ ë†’ì€ ì„ê³„ê°’
        if recall_temp >= 0.6 and f1_temp > best_f1:
            best_threshold = threshold
            best_f1 = f1_temp
    
    print(f"ìµœì  ì„ê³„ê°’: {best_threshold:.2f}")
    
    # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì¬í‰ê°€
    y_pred_optimal = (y_pred_proba >= best_threshold).astype(int)
    
    accuracy_opt = accuracy_score(y_test, y_pred_optimal)
    precision_opt = precision_score(y_test, y_pred_optimal, zero_division=0)
    recall_opt = recall_score(y_test, y_pred_optimal, zero_division=0)
    f1_opt = f1_score(y_test, y_pred_optimal, zero_division=0)
    
    print(f"\nì„ê³„ê°’ {best_threshold:.2f} (ìµœì ):")
    print(f"  Accuracy:  {accuracy_opt:.4f}")
    print(f"  Precision: {precision_opt:.4f}")
    print(f"  Recall:    {recall_opt:.4f} â­")
    print(f"  F1 Score:  {f1_opt:.4f}")
    print(f"  ROC AUC:   {roc_auc:.4f}")
    
    # í˜¼ë™ í–‰ë ¬
    print("\n[9] í˜¼ë™ í–‰ë ¬ (ìµœì  ì„ê³„ê°’):")
    cm = confusion_matrix(y_test, y_pred_optimal)
    print(cm)
    print(f"\nTN: {cm[0,0]} (ì •ìƒì„ ì •ìƒìœ¼ë¡œ)")
    print(f"FP: {cm[0,1]} (ì •ìƒì„ ë¶€ë„ë¡œ - ì˜¤íƒ)")
    print(f"FN: {cm[1,0]} (ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ - ë†“ì¹¨) âš ï¸")
    print(f"TP: {cm[1,1]} (ë¶€ë„ë¥¼ ë¶€ë„ë¡œ - ì •ë‹µ) âœ“")
    
    # ì‹¤ì œ ì˜ë¯¸
    total_bankrupt = y_test.sum()
    detected = cm[1,1]
    missed = cm[1,0]
    
    print(f"\nì‹¤ì œ ë¶€ë„ ê¸°ì—…: {total_bankrupt}ê°œ")
    print(f"âœ“ íƒì§€: {detected}ê°œ ({detected/total_bankrupt*100:.1f}%)")
    print(f"âœ— ë†“ì¹¨: {missed}ê°œ ({missed/total_bankrupt*100:.1f}%)")
    
    # â­â­â­ 10. Feature Importance ë¶„ì„ (Logistic Regression) â­â­â­
    print("\n[10] Feature Importance ë¶„ì„...")
    
    # Logistic Regressionì˜ ê³„ìˆ˜(coefficient) ì¶”ì¶œ
    coefficients = model.coef_[0]  # shape: (n_features,)
    
    # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚° (ì–‘ìˆ˜/ìŒìˆ˜ ëª¨ë‘ ì¤‘ìš”)
    feature_importance_df = pd.DataFrame({
        'feature': final_feature_cols,
        'coefficient': coefficients,
        'abs_coefficient': np.abs(coefficients),
        'importance_rank': None
    })
    
    # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    feature_importance_df = feature_importance_df.sort_values(
        'abs_coefficient', 
        ascending=False
    ).reset_index(drop=True)
    
    # ìˆœìœ„ ë§¤ê¸°ê¸°
    feature_importance_df['importance_rank'] = range(1, len(feature_importance_df) + 1)
    
    # ì˜í–¥ ë°©í–¥ ì¶”ê°€ (ë¶€ë„ ìœ„í—˜ ì¦ê°€ vs ê°ì†Œ)
    feature_importance_df['impact'] = feature_importance_df['coefficient'].apply(
        lambda x: 'ë¶€ë„ ìœ„í—˜ ì¦ê°€ â¬†ï¸' if x > 0 else 'ë¶€ë„ ìœ„í—˜ ê°ì†Œ â¬‡ï¸'
    )
    
    # ì¤‘ìš”ë„ ë¹„ìœ¨ (%) ê³„ì‚°
    total_abs_coef = feature_importance_df['abs_coefficient'].sum()
    feature_importance_df['importance_percent'] = (
        feature_importance_df['abs_coefficient'] / total_abs_coef * 100
    ).round(2)
    
    # Top 20 ì¶œë ¥
    print("\níŠ¹ì„± ì¤‘ìš”ë„ Top 20:")
    print("=" * 100)
    print(f"{'ìˆœìœ„':<5} {'íŠ¹ì„±ëª…':<40} {'ê³„ìˆ˜':<15} {'ì ˆëŒ“ê°’':<15} {'ì˜í–¥':<20} {'ì¤‘ìš”ë„(%)'}")
    print("=" * 100)
    
    for idx, row in feature_importance_df.head(20).iterrows():
        print(f"{row['importance_rank']:<5} {row['feature']:<40} "
              f"{row['coefficient']:<15.6f} {row['abs_coefficient']:<15.6f} "
              f"{row['impact']:<20} {row['importance_percent']:.2f}%")
    
    # ì „ì²´ Feature Importance CSV ì €ì¥
    feature_importance_df.to_csv(
        'feature_importance_logistic.csv', 
        index=False, 
        encoding='utf-8-sig'
    )
    print(f"\nâœ“ ì „ì²´ Feature Importance ì €ì¥: feature_importance_logistic.csv")
    
    # Top 50ë§Œ ë³„ë„ ì €ì¥
    feature_importance_df.head(50).to_csv(
        'feature_importance_top50.csv', 
        index=False, 
        encoding='utf-8-sig'
    )
    print(f"âœ“ Top 50 Feature Importance ì €ì¥: feature_importance_top50.csv")
    
    # ë¶€ë„ ìœ„í—˜ ì¦ê°€ ìš”ì¸ Top 10
    increasing_risk = feature_importance_df[
        feature_importance_df['coefficient'] > 0
    ].head(10)
    
    increasing_risk.to_csv(
        'feature_importance_risk_increasing.csv', 
        index=False, 
        encoding='utf-8-sig'
    )
    print(f"âœ“ ë¶€ë„ ìœ„í—˜ ì¦ê°€ ìš”ì¸ Top 10 ì €ì¥: feature_importance_risk_increasing.csv")
    
    # ë¶€ë„ ìœ„í—˜ ê°ì†Œ ìš”ì¸ Top 10
    decreasing_risk = feature_importance_df[
        feature_importance_df['coefficient'] < 0
    ].head(10)
    
    decreasing_risk.to_csv(
        'feature_importance_risk_decreasing.csv', 
        index=False, 
        encoding='utf-8-sig'
    )
    print(f"âœ“ ë¶€ë„ ìœ„í—˜ ê°ì†Œ ìš”ì¸ Top 10 ì €ì¥: feature_importance_risk_decreasing.csv")
    
    # 11. ëª¨ë¸ ì €ì¥
    print("\n[11] ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì €ì¥í•  ê°ì²´ë“¤
    model_package = {
        'model': model,
        'scaler': scaler,
        'feature_cols': final_feature_cols,
        'optimal_threshold': best_threshold,
        'training_date': timestamp,
        'metrics': {
            'roc_auc': roc_auc,
            'recall': recall_opt,
            'precision': precision_opt,
            'f1_score': f1_opt,
            'accuracy': accuracy_opt
        },
        'feature_importance': feature_importance_df  # â­ Feature Importance ì¶”ê°€
    }
    
    # joblibë¡œ ì €ì¥ (pickleë³´ë‹¤ íš¨ìœ¨ì )
    model_filename = f'bankruptcy_model_{timestamp}.pkl'
    joblib.dump(model_package, model_filename)
    
    print(f"âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_filename}")
    
    # ìµœì‹  ëª¨ë¸ë¡œë„ ì €ì¥ (ë¡œë“œí•˜ê¸° í¸í•˜ê²Œ)
    joblib.dump(model_package, 'bankruptcy_model_latest.pkl')
    print(f"âœ“ ìµœì‹  ëª¨ë¸ ì €ì¥: bankruptcy_model_latest.pkl")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = pd.DataFrame([{
        'timestamp': timestamp,
        'n_samples': len(df),
        'n_features': len(final_feature_cols),
        'default_rate': y.mean(),
        'roc_auc': roc_auc,
        'recall': recall_opt,
        'precision': precision_opt,
        'f1_score': f1_opt,
        'optimal_threshold': best_threshold
    }])
    
    metadata.to_csv('model_metadata.csv', index=False, encoding='utf-8-sig')
    print(f"âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥: model_metadata.csv")
    
    print("\n" + "=" * 80)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  ğŸ“Š ëª¨ë¸ íŒŒì¼:")
    print(f"    - {model_filename}")
    print(f"    - bankruptcy_model_latest.pkl")
    print("  ğŸ“ˆ Feature Importance:")
    print("    - feature_importance_logistic.csv (ì „ì²´)")
    print("    - feature_importance_top50.csv (ìƒìœ„ 50ê°œ)")
    print("    - feature_importance_risk_increasing.csv (ë¶€ë„ ìœ„í—˜ ì¦ê°€ ìš”ì¸)")
    print("    - feature_importance_risk_decreasing.csv (ë¶€ë„ ìœ„í—˜ ê°ì†Œ ìš”ì¸)")
    print("  ğŸ“‹ ë©”íƒ€ë°ì´í„°:")
    print("    - model_metadata.csv")
    
    return model_package


# ============================================================================
# PART 2: ë¶€ë„ í™•ë¥  ì˜ˆì¸¡ í•¨ìˆ˜
# ============================================================================

def predict_bankruptcy_probability(company_data, model_path='bankruptcy_model_latest.pkl'):
    """
    ìƒˆë¡œìš´ ê¸°ì—… ë°ì´í„°ë¡œ ë¶€ë„ í™•ë¥  ì˜ˆì¸¡
    
    Parameters:
    -----------
    company_data : dict or pd.DataFrame
        ê¸°ì—… ì¬ë¬´ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” DataFrame)
    model_path : str
        ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
    --------
    result : dict
        {
            'probability': ë¶€ë„ í™•ë¥  (0~1),
            'risk_level': ìœ„í—˜ ë“±ê¸‰,
            'prediction': ë¶€ë„ ì˜ˆì¸¡ (0 or 1),
            'confidence': ì˜ˆì¸¡ ì‹ ë¢°ë„
        }
    """
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model_package = joblib.load(model_path)
    model = model_package['model']
    scaler = model_package['scaler']
    feature_cols = model_package['feature_cols']
    optimal_threshold = model_package['optimal_threshold']
    
    # 2. ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
    if isinstance(company_data, dict):
        df_input = pd.DataFrame([company_data])
    else:
        df_input = company_data.copy()
    
    # 3. í•„ìš”í•œ íŠ¹ì„±ë§Œ ì¶”ì¶œ
    missing_features = set(feature_cols) - set(df_input.columns)
    if missing_features:
        print(f"ê²½ê³ : ëˆ„ë½ëœ íŠ¹ì„± {len(missing_features)}ê°œë¥¼ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
        for col in missing_features:
            df_input[col] = 0
    
    X_input = df_input[feature_cols].copy()
    
    # 4. ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜, ë¬´í•œëŒ€)
    X_input = X_input.fillna(0)
    X_input = X_input.replace([np.inf, -np.inf], 0)
    
    # 5. ìŠ¤ì¼€ì¼ë§
    X_input_scaled = scaler.transform(X_input)
    
    # 6. ì˜ˆì¸¡
    probability = model.predict_proba(X_input_scaled)[:, 1][0]
    prediction = 1 if probability >= optimal_threshold else 0
    
    # 7. ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜
    if probability < 0.2:
        risk_level = "ë§¤ìš° ë‚®ìŒ"
        risk_color = "ğŸŸ¢"
    elif probability < 0.4:
        risk_level = "ë‚®ìŒ"
        risk_color = "ğŸŸ¡"
    elif probability < 0.6:
        risk_level = "ë³´í†µ"
        risk_color = "ğŸŸ "
    elif probability < 0.8:
        risk_level = "ë†’ìŒ"
        risk_color = "ğŸ”´"
    else:
        risk_level = "ë§¤ìš° ë†’ìŒ"
        risk_color = "ğŸš¨"
    
    # 8. ì‹ ë¢°ë„ ê³„ì‚° (í™•ë¥ ì´ 0 ë˜ëŠ” 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
    confidence = abs(probability - 0.5) * 2  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
    
    # 9. ê²°ê³¼ ë°˜í™˜
    result = {
        'probability': float(probability),
        'probability_percent': f"{probability * 100:.2f}%",
        'prediction': int(prediction),
        'prediction_label': "ë¶€ë„ ì˜ˆìƒ" if prediction == 1 else "ì •ìƒ",
        'risk_level': risk_level,
        'risk_color': risk_color,
        'confidence': float(confidence),
        'threshold': optimal_threshold
    }
    
    return result


# ============================================================================
# PART 3: ë°°ì¹˜ ì˜ˆì¸¡ (ì—¬ëŸ¬ ê¸°ì—… ë™ì‹œ ì˜ˆì¸¡)
# ============================================================================

def predict_batch(df_companies, model_path='bankruptcy_model_latest.pkl'):
    """
    ì—¬ëŸ¬ ê¸°ì—… ë™ì‹œ ì˜ˆì¸¡
    
    Parameters:
    -----------
    df_companies : pd.DataFrame
        ì—¬ëŸ¬ ê¸°ì—…ì˜ ì¬ë¬´ ë°ì´í„°
    model_path : str
        ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
    --------
    df_results : pd.DataFrame
        ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrame
    """
    
    # ëª¨ë¸ ë¡œë“œ
    model_package = joblib.load(model_path)
    model = model_package['model']
    scaler = model_package['scaler']
    feature_cols = model_package['feature_cols']
    optimal_threshold = model_package['optimal_threshold']
    
    # íŠ¹ì„± ì¶”ì¶œ
    missing_features = set(feature_cols) - set(df_companies.columns)
    if missing_features:
        print(f"ê²½ê³ : ëˆ„ë½ëœ íŠ¹ì„± {len(missing_features)}ê°œë¥¼ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
        for col in missing_features:
            df_companies[col] = 0
    
    X = df_companies[feature_cols].copy()
    
    # ì „ì²˜ë¦¬
    X = X.fillna(0)
    X = X.replace([np.inf, -np.inf], 0)
    
    # ìŠ¤ì¼€ì¼ë§
    X_scaled = scaler.transform(X)
    
    # ì˜ˆì¸¡
    probabilities = model.predict_proba(X_scaled)[:, 1]
    predictions = (probabilities >= optimal_threshold).astype(int)
    
    # ìœ„í—˜ ë“±ê¸‰
    risk_levels = []
    for prob in probabilities:
        if prob < 0.2:
            risk_levels.append("ë§¤ìš° ë‚®ìŒ")
        elif prob < 0.4:
            risk_levels.append("ë‚®ìŒ")
        elif prob < 0.6:
            risk_levels.append("ë³´í†µ")
        elif prob < 0.8:
            risk_levels.append("ë†’ìŒ")
        else:
            risk_levels.append("ë§¤ìš° ë†’ìŒ")
    
    # ê²°ê³¼ ì¶”ê°€
    df_results = df_companies.copy()
    df_results['ë¶€ë„í™•ë¥ '] = probabilities
    df_results['ë¶€ë„í™•ë¥ (%)'] = (probabilities * 100).round(2)
    df_results['ë¶€ë„ì˜ˆì¸¡'] = predictions
    df_results['ìœ„í—˜ë“±ê¸‰'] = risk_levels
    
    return df_results


# ============================================================================
# PART 4: ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    
    # ========================================
    # 1. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ (ìµœì´ˆ 1íšŒë§Œ)
    # ========================================
    
    print("\n" + "=" * 80)
    print("ëª¨ë¸ì„ í•™ìŠµí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
    print("(ìµœì´ˆ ì‹¤í–‰ ë˜ëŠ” ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ 'y' ì…ë ¥)")
    print("=" * 80)
    
    choice = input("ì…ë ¥: ").strip().lower()
    
    if choice == 'y':
        model_package = train_and_save_model()
        print("\nâœ“ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
    
    # ========================================
    # 2. CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì„œ ì˜ˆì¸¡ â­ ìƒˆë¡œ ì¶”ê°€!
    # ========================================
    
    print("\n" + "=" * 80)
    print("[PART 2] CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ì–´ì„œ ì˜ˆì¸¡")
    print("=" * 80)
    
    csv_file = r'C:\Users\user\rde-data\test.csv'  # ë˜ëŠ” ì‚¬ìš©ì ì…ë ¥ë°›ê¸°
    
    try:
        # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(csv_file):
            print(f"\nâš ï¸  '{csv_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            print(f"\nâœ“ '{csv_file}' íŒŒì¼ì„ ì½ëŠ” ì¤‘...")
            
            # CSV íŒŒì¼ ì½ê¸° (íƒ­ êµ¬ë¶„ì ì‹œë„)
            try:
                df_test = pd.read_csv(csv_file, sep='\t')
                print(f"âœ“ íƒ­ êµ¬ë¶„ìë¡œ ì½ê¸° ì„±ê³µ")
            except:
                # ì‰¼í‘œ êµ¬ë¶„ì ì‹œë„
                df_test = pd.read_csv(csv_file)
                print(f"âœ“ ì‰¼í‘œ êµ¬ë¶„ìë¡œ ì½ê¸° ì„±ê³µ")
            
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_test)}ê°œ ê¸°ì—…")
            print(f"âœ“ ì»¬ëŸ¼ ìˆ˜: {len(df_test.columns)}")
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            print("\nì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
            result = predict_bankruptcy_probability(df_test.iloc[0].to_dict())
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "-" * 80)
            print("ì˜ˆì¸¡ ê²°ê³¼:")
            print("-" * 80)
            print(f"ê¸°ì—…ëª…/ID:     (CSV 1ë²ˆì§¸ í–‰)")
            print(f"ë¶€ë„ í™•ë¥ :     {result['probability_percent']} {result['risk_color']}")
            print(f"ìœ„í—˜ ë“±ê¸‰:     {result['risk_level']}")
            print(f"ì˜ˆì¸¡ ê²°ê³¼:     {result['prediction_label']}")
            print(f"ì˜ˆì¸¡ ì‹ ë¢°ë„:   {result['confidence']:.2%}")
            print(f"ì‚¬ìš© ì„ê³„ê°’:   {result['threshold']:.2f}")
            print("-" * 80)
            
            if result['prediction'] == 1:
                print("\nâš ï¸  ì´ ê¸°ì—…ì€ í–¥í›„ 1ë…„ ë‚´ ë¶€ë„ ìœ„í—˜ì´ ë†’ìŠµë‹ˆë‹¤!")
                print("    ì¶”ê°€ ì‹¬ì‚¬ ë˜ëŠ” ëŒ€ì¶œ ê±°ì ˆì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                print("\nâœ“  ì´ ê¸°ì—…ì€ ì¬ë¬´ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤.")
                print("    ëŒ€ì¶œ ìŠ¹ì¸ ê°€ëŠ¥ (ë‹¨, ì¶”ê°€ ê²€í†  ê¶Œì¥)")
            
            # ì—¬ëŸ¬ ê¸°ì—… ì˜ˆì¸¡ ì˜µì…˜
            if len(df_test) > 1:
                print("\n" + "-" * 80)
                print(f"CSV íŒŒì¼ì— {len(df_test)}ê°œ ê¸°ì—…ì´ ìˆìŠµë‹ˆë‹¤.")
                print("ëª¨ë“  ê¸°ì—…ì„ ì˜ˆì¸¡í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
                print("-" * 80)
                
                batch_choice = input("ì…ë ¥: ").strip().lower()
                
                if batch_choice == 'y':
                    print("\në°°ì¹˜ ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
                    df_results = predict_batch(df_test)
                    
                    # ê²°ê³¼ ì €ì¥
                    output_file = 'predictions_result.csv'
                    df_results[['ë¶€ë„í™•ë¥ (%)', 'ë¶€ë„ì˜ˆì¸¡', 'ìœ„í—˜ë“±ê¸‰']].to_csv(
                        output_file, 
                        index=False, 
                        encoding='utf-8-sig'
                    )
                    
                    print(f"\nâœ“ ì˜ˆì¸¡ ì™„ë£Œ: {len(df_results)}ê°œ ê¸°ì—…")
                    print(f"âœ“ ê²°ê³¼ ì €ì¥: {output_file}")
                    
                    # ìš”ì•½ í†µê³„
                    print("\n" + "-" * 80)
                    print("ì˜ˆì¸¡ ìš”ì•½:")
                    print("-" * 80)
                    print(f"ì „ì²´ ê¸°ì—… ìˆ˜:     {len(df_results)}")
                    print(f"ë¶€ë„ ì˜ˆìƒ:        {df_results['ë¶€ë„ì˜ˆì¸¡'].sum()}ê°œ ({df_results['ë¶€ë„ì˜ˆì¸¡'].sum()/len(df_results)*100:.1f}%)")
                    print(f"ì •ìƒ ì˜ˆìƒ:        {(df_results['ë¶€ë„ì˜ˆì¸¡']==0).sum()}ê°œ ({(df_results['ë¶€ë„ì˜ˆì¸¡']==0).sum()/len(df_results)*100:.1f}%)")
                    print(f"\ní‰ê·  ë¶€ë„ í™•ë¥ :   {df_results['ë¶€ë„í™•ë¥ (%)'].mean():.2f}%")
                    print(f"ìµœê³  ë¶€ë„ í™•ë¥ :   {df_results['ë¶€ë„í™•ë¥ (%)'].max():.2f}%")
                    print(f"ìµœì € ë¶€ë„ í™•ë¥ :   {df_results['ë¶€ë„í™•ë¥ (%)'].min():.2f}%")
                    print("-" * 80)
                    
                    # ìœ„í—˜ ë“±ê¸‰ë³„ ë¶„í¬
                    print("\nìœ„í—˜ ë“±ê¸‰ ë¶„í¬:")
                    print(df_results['ìœ„í—˜ë“±ê¸‰'].value_counts().sort_index())
    
    except FileNotFoundError:
        print("\nâŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”. (í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ 'y' ì…ë ¥)")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

    # ========================================
    # 4. í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    # ========================================
    
    print("\n" + "=" * 80)
    print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("=" * 80)